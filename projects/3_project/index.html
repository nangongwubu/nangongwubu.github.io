<!DOCTYPE html>
<html>

  <head>
    <meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta http-equiv="X-UA-Compatible" content="IE=edge">

<title>Shi Gu | Artificial Intelligence</title>
<meta name="description" content="A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design.
">

<!-- Open Graph -->


<!-- Bootstrap & MDB -->
<link href="https://stackpath.bootstrapcdn.com/bootstrap/4.5.2/css/bootstrap.min.css" rel="stylesheet" integrity="sha512-MoRNloxbStBcD8z3M/2BmnT+rg4IsMxPkXaGh2zD6LGNNFE80W3onsAhRcMAMrSoyWL9xD7Ert0men7vR8LUZg==" crossorigin="anonymous">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/mdbootstrap/4.19.1/css/mdb.min.css" integrity="sha512-RO38pBRxYH3SoOprtPTD86JFOclM51/XTIdEPh5j8sj4tp8jmQIx26twG52UaLi//hQldfrh7e51WzP9wuP32Q==" crossorigin="anonymous" />

<!-- Fonts & Icons -->
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.14.0/css/all.min.css"  integrity="sha512-1PKOgIY59xJ8Co8+NE6FZ+LOAZKjy+KY8iq0G4B3CyeY6wYHN3yt9PW0XpSriVlkMXe40PTKnXrLnZ9+fkDaog==" crossorigin="anonymous">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/academicons/1.9.0/css/academicons.min.css" integrity="sha512-W4yqoT1+8NLkinBLBZko+dFB2ZbHsYLDdr50VElllRcNt2Q4/GSs6u71UHKxB7S6JEMCp5Ve4xjh3eGQl/HRvg==" crossorigin="anonymous">
<link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons">

<!-- Code Syntax Highlighting -->
<link rel="stylesheet" href="https://gitcdn.xyz/repo/jwarby/jekyll-pygments-themes/master/github.css" />

<!-- Styles -->

<link rel="icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>ðŸ§ </text></svg>">

<link rel="stylesheet" href="/assets/css/main.css">

<link rel="canonical" href="/projects/3_project/">

<!-- JQuery -->
<!-- jQuery -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js" integrity="sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==" crossorigin="anonymous"></script>


<!-- Theming-->

  <script src="/assets/js/theme.js"></script>
  <!-- Load DarkMode JS -->
<script src="/assets/js/dark_mode.js"></script>






    
<!-- MathJax -->
<script type="text/javascript">
  window.MathJax = {
    tex: {
      tags: 'ams'
    }
  };
</script>
<script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.1.2/es5/tex-mml-chtml.js"></script>
<script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>


  </head>

  <body class="fixed-top-nav ">

    <!-- Header -->

    <header>

    <!-- Nav Bar -->
    <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top">
    <div class="container">
      
      <a class="navbar-brand title font-weight-lighter" href="/">
       Shi Gu
      </a>
      
      <!-- Navbar Toggle -->
      <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
        <span class="sr-only">Toggle navigation</span>
        <span class="icon-bar top-bar"></span>
        <span class="icon-bar middle-bar"></span>
        <span class="icon-bar bottom-bar"></span>
      </button>
      <div class="collapse navbar-collapse text-right" id="navbarNav">
        <ul class="navbar-nav ml-auto flex-nowrap">
          <!-- About -->
          <li class="nav-item ">
            <a class="nav-link" href="/">
              about
              
            </a>
          </li>
          <!-- Other pages -->
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          <li class="nav-item ">
              <a class="nav-link" href="/projects/">
                projects
                
              </a>
          </li>
          
          
          
          <li class="nav-item ">
              <a class="nav-link" href="/publications/">
                publications
                
              </a>
          </li>
          
          
          
          <li class="nav-item ">
              <a class="nav-link" href="/teaching/">
                teaching
                
              </a>
          </li>
          
          
          
          <li class="nav-item ">
              <a class="nav-link" href="/team/">
                team
                
              </a>
          </li>
          
          
          
          <!-- Blog -->
          <li class="nav-item ">
            <a class="nav-link" href="/blog/">
              blog
              
            </a>
          </li>
          
          
            <div class = "toggle-container">
              <a id = "light-toggle">
                  <i class="fas fa-moon"></i>
                  <i class="fas fa-sun"></i>
              </a>
            </div>
          
        </ul>
      </div>
    </div>
  </nav>

</header>


    <!-- Content -->

    <div class="container mt-5">
      <div class="post">

  <header class="post-header">
    <h1 class="post-title">Artificial Intelligence</h1>
    <p class="post-description">We focus on the spiking neural networks and medical imaging analysis.</p>
  </header>

  <article>
    <p>For further interest, please refer to <a href="https://openreview.net/forum?id=FZ1oTwcXchK">Deng et. al. 2021, ICLR</a> and <a href="https://arxiv.org/abs/2106.06984">Li et. al. 2021, ICML</a>.</p>

<h2 id="snn-model">SNN model</h2>
<p>Spiking neuron networks (SNNs) are biologically inspired networks, which have received increasing attention due to their efficient computation. During their development, many mathematical models have been proposed to describe neuron behavior. The most widely used neuron model for SNN is the Leaky Integrate-and-Fire (LIF) model, which uses simple differential equations to describe the membrane potential  behavior of neurons. Its format of explicit iteration is governed by</p>

\[v_{temp}(t+1)=\alpha*v(t)+Ws(t)\]

<p>where \(v\) and \(v_{temp}\) is the membrane potential, \(\alpha\) means the decay factor, \(W\) denotes the synapse weight, and $s$ denotes the spike input at time t.</p>

<p>When the membrane potential exceeds the pre-defined threshold \(V_{th}\), it would produce a spike output \(\theta\), and the membrane potential will decrease by two reset mechanisms:</p>

<p>soft reset: \(v(t+1)=v_{temp}(t+1)-V_{th}\)<br />
hard reset: \(v(t+1)=v_{temp}(t+1)*(1-\theta)\)</p>

<h2 id="works">works</h2>
<p>The major bottleneck of the spiking neuron network is how to acquire a well-performance SNN, especially on a complex dataset, since directly using the backpropagation algorithm is not suitable for SNN.<br />
Currently, two ways can effectively obtain excellent SNNs: surrogate gradient and ANN to SNN. ANN to SNN means converting a well-performed source ANN to a target SNN. Surrogate gradient methods use a soft relaxed function to replace the hard step function and train SNN by BPTT.</p>

<h3 id="ann-to-snn">ANN to SNN</h3>
<p>In this subsection, We use the soft reset and IF model (\(\alpha=1\)) to reduce the information loss between the source ANN and target SNN. Our approach is based on threshold-balancing method, which contain three steps:</p>
<ol>
  <li>Train the source ANN that only contains Convolutional, average pooling, fully connected layer, and ReLU activation function. Record the maximum activation of each layer.</li>
  <li>Copy the network parameter to target SNN and replace the ReLU function with the IF model. The threshold of each layer is setting to the maximum activation.</li>
  <li>Run the target SNN with enough simulation length to achieve acceptable accuracy.</li>
</ol>

<p>The converted SNN needs thousands of simulation time to achieve the same accuracy as source ANN, which does not meet the high-efficiency characteristics. So our work is to explore how to obtain higher accuracy, and lower inference latency converted SNN.</p>

<h4 id="layer-wise-conversion-error"><a href="https://openreview.net/forum?id=FZ1oTwcXchK">Layer-wise conversion error</a></h4>
<p>We split the conversion error into clipping error and flooring error. When the source ANN activation is larger than \(V_{th}\), 
the SNN neuronâ€™s output is smaller than the output of the source neuron (clipping). And the reason for the flooring error is that
SNN canâ€™t send accurate values to the next layer. The threshold balancing method can eliminate the clipping error since 
it uses maximum activation as the threshold. However, it also will increase the flooring error, which can be reduced by
increasing simulation length. Here, we first find an equation to approximate the target SNNâ€™s spike frequency:</p>

\[\bar{s}^{l+1} = V_{th}/T \cdot clip(\left \lfloor (T/V_{th}) \cdot W^l \bar{s}^l,0,T \right \rfloor)\]

<p>Though analyze the output error between the source ANN and converted SNN, we decompose the conversion error into the output error of each layer. As a result, we can make the converted SNN closer to the source ANN by simply reducing the output error of each layer. Here we propose a method to reduce the flooring error: only to increase the SNNâ€™s bias by \(V_{th}/2T\).</p>

<figure class="highlight"><pre><code class="language-ruby" data-lang="ruby"><span class="k">for</span> <span class="n">l</span><span class="o">=</span><span class="mi">1</span> <span class="n">to</span> <span class="no">L</span> <span class="k">do</span> 
  <span class="no">SNN</span><span class="p">.</span><span class="nf">layer</span><span class="p">[</span><span class="n">l</span><span class="p">].</span><span class="nf">thresh</span><span class="o">&lt;-</span><span class="no">ANN</span><span class="p">.</span><span class="nf">layer</span><span class="p">[</span><span class="n">l</span><span class="p">].</span><span class="nf">maximum_activation</span>
  <span class="no">SNN</span><span class="p">.</span><span class="nf">layer</span><span class="p">[</span><span class="n">l</span><span class="p">].</span><span class="nf">weight</span><span class="o">&lt;-</span><span class="no">ANN</span><span class="p">.</span><span class="nf">layer</span><span class="p">[</span><span class="n">l</span><span class="p">].</span><span class="nf">weight</span>
  <span class="no">SNN</span><span class="p">.</span><span class="nf">layer</span><span class="p">[</span><span class="n">l</span><span class="p">].</span><span class="nf">bias</span><span class="o">&lt;-</span><span class="no">ANN</span><span class="p">.</span><span class="nf">layer</span><span class="p">[</span><span class="n">l</span><span class="p">].</span><span class="nf">bias</span> <span class="o">+</span> <span class="no">SNN</span><span class="p">.</span><span class="nf">layer</span><span class="p">[</span><span class="n">l</span><span class="p">].</span><span class="nf">thresh</span> <span class="o">/</span> <span class="p">(</span><span class="mi">2</span><span class="o">*</span><span class="no">T</span><span class="p">)</span>
<span class="k">end</span> <span class="k">for</span></code></pre></figure>

<h4 id="layer-wise-caibration"><a href="https://arxiv.org/abs/2106.06984">Layer-wise Caibration</a></h4>

<p><strong>Adaptive threshold.</strong> We found that threshold balancing will cause a considerable flooring error, 
especially when the simulation length is not enough, since it uses the maximum activation as the SNNâ€™s threshold. 
In practice, In practice, an Appropriate reduction of the threshold will increase the SNN performance.<br />
Here, we minimize the optimization problem, which is formulated by</p>

\[\min_{V_{th}} (clipfloor(x^l,T,V_{th}^l)-ReLU(x^l)).\]

<p>We use a grid search with linarly sample N grids between $[0, max(x^l)]$ to determine the final result of \(V_{th}^l\).</p>

<p>The previous method does not rely on real data statistics. It is made by a strong assumption that activation is uniformly 
distributed. In fact, we can get a better bias increment by analyzing the distribution of some training samples. 
Here, we propose two methods to calibrate the SNNâ€™s bias and weight, respectively layer-by-layer.</p>

<p><strong>Bias correction (BC).</strong> In order to calibrate the bias, we first define a reduced mean function:</p>

\[\mu_c(x)=\frac{1}{wh}\sum_{i=1}^{w}\sum_{j=1}^{h}x_{c,i,j}\]

<p>where w,h are the width and height of the feature-map, so \(\mu_c(x)\) computes the spatial mean of the feature-map 
in each channel c. The spatial mean of conversion error can be written by:</p>

\[\mu_c (e^l) = \mu_c (x^l)-\mu_c(\bar{s}^l)\]

<p>Then we can add the expected conversion error into the bias term as \(b^l \leftarrow b^l+\mu_c(e^l)\).</p>

<p><strong>Potential correction (PC).</strong> Potential is similar to bias correction. In this method we can directly 
set \(v^l (0)\) to \(T \times e^l\) to calibrate the initial potential.</p>

<figure class="highlight"><pre><code class="language-ruby" data-lang="ruby"><span class="k">for</span> <span class="n">l</span><span class="o">=</span><span class="mi">1</span> <span class="n">to</span> <span class="no">L</span> <span class="k">do</span>
  <span class="no">SNN</span><span class="p">.</span><span class="nf">layer</span><span class="p">[</span><span class="n">l</span><span class="p">].</span><span class="nf">frequency</span> <span class="o">=</span> <span class="no">SNN</span><span class="p">.</span><span class="nf">layer</span><span class="p">[</span><span class="n">l</span><span class="p">].</span><span class="nf">output</span><span class="p">.</span><span class="nf">sum</span><span class="p">(</span><span class="mi">4</span><span class="p">)</span> <span class="o">/</span> <span class="no">T</span>
  <span class="no">Layer</span><span class="p">[</span><span class="n">l</span><span class="p">]</span><span class="o">.</span><span class="no">Error</span> <span class="o">=</span> <span class="no">ANN</span><span class="p">.</span><span class="nf">layer</span><span class="p">[</span><span class="n">l</span><span class="p">].</span><span class="nf">output</span> <span class="o">-</span> <span class="no">SNN</span><span class="p">.</span><span class="nf">layer</span><span class="p">[</span><span class="n">l</span><span class="p">].</span><span class="nf">frequency</span>
  <span class="no">SNN</span><span class="p">.</span><span class="nf">layer</span><span class="p">[</span><span class="n">l</span><span class="p">].</span><span class="nf">bias</span> <span class="o">&lt;-</span> <span class="no">SNN</span><span class="p">.</span><span class="nf">layer</span><span class="p">[</span><span class="n">l</span><span class="p">].</span><span class="nf">bias</span> <span class="o">+</span> <span class="no">Layer</span><span class="p">[</span><span class="n">l</span><span class="p">]</span><span class="o">.</span><span class="no">Error</span><span class="p">.</span><span class="nf">mean</span><span class="p">(</span><span class="mi">0</span><span class="p">).</span><span class="nf">mean</span><span class="p">(</span><span class="mi">2</span><span class="p">).</span><span class="nf">mean</span><span class="p">(</span><span class="mi">3</span><span class="p">)</span> <span class="c1"># bias correct</span>
  <span class="no">SNN</span><span class="p">.</span><span class="nf">layer</span><span class="p">[</span><span class="n">l</span><span class="p">].</span><span class="nf">mem</span> <span class="o">&lt;-</span> <span class="no">SNN</span><span class="p">.</span><span class="nf">layer</span><span class="p">[</span><span class="n">l</span><span class="p">].</span><span class="nf">mean</span> <span class="o">+</span>  <span class="no">Layer</span><span class="p">[</span><span class="n">l</span><span class="p">]</span><span class="o">.</span><span class="no">Error</span><span class="p">.</span><span class="nf">mean</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span> <span class="c1"># potential correct  </span></code></pre></figure>

<p><strong>Weight calibration (WC).</strong> The layer-wise conversion can be written as \(e^l = x^l - \bar{s}^l\). Then we need to minimize the formulation below:</p>

\[\min_{w^l} \left \| e^l \right \|^2\]

<p>via stochastic gradient descent.</p>

<h3 id="surrogate-gradient">Surrogate gradient</h3>
<p>Surrogate gradient methods use a soft relaxed function to replace the hard step function and train SNN by BPTT. 
There are many shapes of surrogate gradients, like rectangles, exponential, and triangles. 
But during the training process, the surrogate gradient is always optimal?</p>

<h4 id="dspike-under-review">[Dspike]: under review</h4>
<p>Here we propose a new family of Differentiable Spike (Dspike) functions that can adaptively evolve during training to 
find the optimal shape and smoothness for gradient estimation. Mathmatically, Dspike function is like:</p>

\[Dspike(x,b)=\frac{tanh(b(x-V_{th}))+tanh(b/2)}{2(tanh(b/2))}\]

<p>where b is the temperature to adjust the shape of the surrogate gradient function. Specifically we have</p>

<p>\(\lim_{b\rightarrow 0_+} Dspike(x)\rightarrow x\) and \(\lim_{b\rightarrow \infty_+} Dspike(x)\rightarrow sign(x-V_{th})\).</p>

<p>Then we calculate the finite difference gradient (FDG) of the first layerâ€™s weight with a small constant step \(\varepsilon\). 
For each weight value of first layer, we have:</p>

\[\bigtriangledown_{\varepsilon,w_i}L = \frac{L(w_i+\varepsilon e_i)-L(w_i - \varepsilon e_i)}{2\varepsilon}\]

<p>At each epoch, we use a batchsize training sample for our Dspike method to compute their FDG of first layerâ€™s weight. 
Then we calculate the three true gradients by BPTT under the situation: \(b\), \(b+\delta b\), and \(b-\delta b\), 
respectively. Next, we will compare the cosie similarity between the true gradient and the FDG, and adjust 
the temperature of Dspike as the best \(b\).</p>


  </article>

</div>

    </div>

    <!-- Footer -->

    
<footer class="fixed-bottom">
  <div class="container mt-0">
    &copy; Copyright 2021 Shi  Gu.
    Powered by <a href="http://jekyllrb.com/" target="_blank">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank">Unsplash</a>.

    
    
  </div>
</footer>



  </body>

  <!-- Bootsrap & MDB scripts -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/2.4.4/umd/popper.min.js" integrity="sha512-eUQ9hGdLjBjY3F41CScH3UX+4JDSI9zXeroz7hJ+RteoCaY+GP/LDoM8AO+Pt+DRFw3nXqsjh9Zsts8hnYv8/A==" crossorigin="anonymous"></script>
<script src="https://stackpath.bootstrapcdn.com/bootstrap/4.5.2/js/bootstrap.min.js" integrity="sha512-M5KW3ztuIICmVIhjSqXe01oV2bpe248gOxqmlcYrEzAvws7Pw3z6BK0iGbrwvdrUQUhi3eXgtxp5I8PDo9YfjQ==" crossorigin="anonymous"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/mdbootstrap/4.19.1/js/mdb.min.js" integrity="sha512-Mug9KHKmroQFMLm93zGrjhibM2z2Obg9l6qFG2qKjXEXkMp/VDkI4uju9m4QKPjWSwQ6O2qzZEnJDEeCw0Blcw==" crossorigin="anonymous"></script>

  
<!-- Mansory & imagesLoaded -->
<script defer src="https://unpkg.com/masonry-layout@4/dist/masonry.pkgd.min.js"></script>
<script defer src="https://unpkg.com/imagesloaded@4/imagesloaded.pkgd.min.js"></script>
<script defer src="/assets/js/mansory.js" type="text/javascript"></script>


  


<!-- Load Common JS -->
<script src="/assets/js/common.js"></script>


</html>
